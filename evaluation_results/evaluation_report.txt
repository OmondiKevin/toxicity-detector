================================================================================
MULTILABEL CLASSIFICATION - MODEL EVALUATION REPORT
================================================================================

LSTM MODEL RESULTS
--------------------------------------------------------------------------------
Label                 Precision     Recall   F1-Score    ROC-AUC    Support
--------------------------------------------------------------------------------
toxic                    0.6523     0.6852     0.6684     0.8198       1528
severe_toxic             0.6190     0.0304     0.0580     0.8169        427
obscene                  0.4914     0.3633     0.4178     0.7392       1101
threat                   0.0000     0.0000     0.0000        nan          0
insult                   0.6516     0.6878     0.6692     0.8198       1528
identity_hate            0.4444     0.0094     0.0183     0.8167        427
non_offensive            0.8129     0.7934     0.8030     0.8198       2672
--------------------------------------------------------------------------------
Macro Avg                0.5245     0.3671     0.3764     0.8054
Micro Avg                0.6949     0.6033     0.6459

BERT MODEL RESULTS
--------------------------------------------------------------------------------
Label                 Precision     Recall   F1-Score    ROC-AUC    Support
--------------------------------------------------------------------------------
toxic                    0.8224     0.4607     0.5906     0.8650       1528
severe_toxic             0.7407     0.0468     0.0881     0.8603        427
obscene                  0.5313     0.2234     0.3146     0.7790       1101
threat                   0.0000     0.0000     0.0000        nan          0
insult                   0.8192     0.4804     0.6056     0.8649       1528
identity_hate            0.6800     0.0398     0.0752     0.8605        427
non_offensive            0.7607     0.9386     0.8403     0.8653       2672
--------------------------------------------------------------------------------
Macro Avg                0.6221     0.3128     0.3592     0.8492
Micro Avg                0.7601     0.5504     0.6385

MODEL COMPARISON SUMMARY
--------------------------------------------------------------------------------
Metric                                    LSTM            BERT          Winner
--------------------------------------------------------------------------------
Macro F1-Score                          0.3764          0.3592            LSTM
Macro Precision                         0.5245          0.6221            BERT
Macro Recall                            0.3671          0.3128            LSTM
Macro ROC-AUC                           0.8054          0.8492            BERT
Micro F1-Score                          0.6459          0.6385            LSTM
================================================================================